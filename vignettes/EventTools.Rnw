\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage[firstpage]{draftwatermark}
\title{EventTools: Non-proportional hazards and lagtimes}
\author{Daniel Dalevi}
% \VignetteIndexEntry{Predict from data - Basic tutorial}
%\VignetteEngine{knitr::knitr} 

\SetWatermarkText{DRAFT: UNDER DEVELOPMENT}
\SetWatermarkLightness{0.75}
\SetWatermarkScale{2}

\begin{document}
\sloppy

<<include=FALSE>>=
library(knitr)
opts_chunk$set(
concordance=TRUE
)
@

\maketitle

\section{Introduction}
The eventTools package contains some extensions of functionality for the eventPrediction package. It will use the same interface when possible and the idea is to mimic the command-sequences used and propagate the functionality (accrual processes, dropouts etc). The first add in is a method for dealing with non-proportional hazards and lag-times in Predict from data, available in version 1.0. The plan is to include more functions to this package with iterative releases. 

\section{Preliminary steps}
Before starting this tutorial you will need to install the eventPrediction and the eventTools packages and their dependencies. For all examples below, ensure you load the libraries first. 
<<load,message=FALSE>>=
library( eventPrediction )
library( eventTools )
@

\section{Estimating the change-point}
The data used in this vignette is simulated under the assumption of a lag-time. The same approach can, however, be used for other data where there is an effect of non-proportional hazards with a change-point at a specific time (A 2-piecewise model). In the case of a lag-effect, the HR is $1$ until the change-point when treatment starts showing effect. The simulated OS data set is available in the package and we will be used for illustration. It contains $979$ subjects and assumes a lag-time of $165$ days and a post-lag HR of $0.25$. The control median was set to $5.8$ with a shape parameter of $1.2$. The study settings (not the simulated data) can be visualized using the \texttt{nonproportionalHazards} package (for details see the documentation of the package). 
<<echo=FALSE, fig=TRUE>>=
library( nonproportionalHazards)
study <- delayedResponse( lag.time=5.5, 
                          median.control.time=5.8, 
                          hazard.ratio=0.25,
                          followup.time=20, 
                          total.patients=979, 
                          alpha=0.05,
                          randomization=1, 
                          accrual.duration=12, 
                          accrual.weight=1,
                          definition=hazard )
    landmark.times <- seq(0, 32, length.out=100)
    curves <- survival( study, landmark.times )
    plot( curves, xlab="Study time [months]" )
@
The dataset is loaded into R and an EventData object is created. The time is calculated from the event date and the last date. 
<<>>=
data( lag.data )

my.data <- EventData( data=lag.data,
                      subject="subject",
                      rand.date="randDate",
                      has.event="hasEvent",
                      withdrawn="withdrawn",
                      time=list( event.date="eventDate",
                                 last.date="lastDate" ) )
@
A model can be fitted to this data (see EventPrediction vignette) without accounting for the lag-effect. The survival curve can be plotted with the model. 
<<>>=
my.fit <- fit( my.data )
plot( my.fit )
@

It is evident that the model fit is poor which is not surprising due to the nature of the data. Assuming that we have a change-point ($T$), there are various ways of estimating when this occurs. Perhaps the simplest is to include a co-variate (factor) in the survival method which is true if a subject is in the lag period and false otherwise. With an exponential survival function we can model the event rate as if constant before and after T. Given a set of possible change-points we can select the $T$ providing the best model fit. This may not always converge but in the package there is a function that visualize the fit given a search boundary for $T$. Note, the exponential survival function is set as default because it easily converges with a single parameter. Other distributions can also be selected. It is also important to note that currently no tests are made for the presence of a lag-effect. The method may give a change-point estimate though there is none and therefore it is also important to visually inspect the data (see also Section~\ref{sec:leftfit})   
<<>>=
  est.obj <- estimateLagTime( my.data, t.start=0.5, t.stop=20, dt=0.5 )
  plot( est.obj )
  estLagT <- getEstimate( est.obj )
@


\section{Right censor data based on the KM curve}
Different survival functions will be fitted before and after the estimated change-point dividing it into two parts. Since time in this case is subject specific, it will be relative to when subjects entered the study. The first part of the interval $[0,T)$ will be equivalent to a study with a fixed follow-up period of $T$ meaning that no subject is studied longer than $T$. We will refer to this data (and model) as \emph{KM right censored}. The package allows for cutting the data in this way using the function \texttt{KMRightCensor}. Then the normal \texttt{fit} function of the eventPrediction package will be used.
 
<<>>=
right.cens.data <- KMRightCensor( my.data, estLagT )
right.cens.fit <- fit( right.cens.data )
plot( right.cens.fit, ylim=c(0.5,1) )
@

The Weibull parameters can be obtained from the fitted object.
<<>>=
right.cens.fit@simParams@parameters
@

This can be compared to the original fit.
<<>>=
my.fit@simParams@parameters
@

\section{Fit a model on left truncated data}
\label{sec:leftfit}
The second part of the data will only contain subjects that survived longer than the change-point $[T,T_{analysis}]$ where $T_{analysis}$ is the time, relative to randomization, that we censored the subjects. This data (and model) will be referred to as \emph{left-truncated} as the subject has been at risk before entering the interval; it is conditioned that they have not had an event. The \texttt{LeftFit} function performs the necessary steps by first removing subjects with $t<T$ and then it fits the Weibull model. The estimation is done with the \texttt{weibreg} function from the \texttt{eha} package which is used internally, see vignette for more details and \cite{Brostrom:2012}.  

<<>>=
 left.trunc.fit <- LeftFit( my.data, estLagT )
 left.trunc.fit@simParams@parameters
 plot( right.cens.fit, left.trunc.fit )
@

While the fit is much better than the original one, we can see that the tail may not be fully captured. Depending on the censoring status of the data and other aspects it may be hard to entirely automate the estimation of the change point. If for example you have a delay in reporting it may be expected that the tail is a bit off. Therefore it is important also to visually inspect the resulting curves. The implemented approach allows us to adjust the change-point and refit the models. In this case the tail may be better modelled by increasing it from $152$ to $160$ days. In later versions (See Section~\ref{sec:futwork}) we aim to include an estimate of a CI for the change-point which would give an indication of the plausible value range.

<<>>=
 estLagT <- 160 
 right.cens.data <- KMRightCensor( my.data, estLagT )
 right.cens.fit <- fit( right.cens.data )
 left.trunc.fit <- LeftFit( my.data, estLagT )
 plot( right.cens.fit, left.trunc.fit )
@

\section{Event Prediction}
We use an updated simulation procedure to incorporate the contribution from both models. For all patients, that have not had events and been in the trial for time $t$, we simulate events by: 
\begin{enumerate}
\item If $t<T$ draw an event time $t_{rc}$ from the right-censored model. 
\begin{enumerate}
	\item If $t_{rc}<T$ this will be recorded as the event time.
  \item	If $t_{rc} \ge T$, draw a new event time $t_{rc,lt}$ from the left-truncated model conditioned on $T$. $t_{rc,lt}$ will be the recorded event time.  
\end{enumerate}
\item If $t \ge T$ draw an event time $t_{lt}$ from the left-truncated model. This will be the event time. 
\end{enumerate}

This is iterated over a number of simulations (\texttt{Nsim}). The \texttt{simulatePW} function will accept the same arguments as \texttt{simulate} in the \texttt{eventPrediction} package. For example, adding new subjects (if still recruiting) can be achieved by adding the argument \texttt{accrualGenerator}. 

<<>>=
  results <- simulatePW( right.cens.fit, left.trunc.fit,  
    Nsim = 500, #Number of simulations to perform
    seed = 20160322 ) #A random seed for reproducibility
  results <- predict( results, event.pred=900 )
  plot( results, show.title=TRUE) 
@

The expected time when reaching $900$ events for this data is $39$ months, i.e. April 2017. This can be compared with the prediction obtained from the original model (using a singel Weibull) which underestimates the data, April is outside the CI.

<<>>=
  results <- simulate( my.fit,  
    Nsim = 500, #Number of simulations to perform
    seed = 20160322 ) #A random seed for reproducibility
  results <- predict( results, event.pred=900 )
  plot( results, show.title=TRUE )
@

\section{Future work}
\label{sec:futwork}
No error estimates are provided for the estimation of the change-point. This can be achieved by non-parametric bootstrapping. This will help in interpreting how precise the estimate is and also whether an effect is present or not. We are also looking into a mixture model that may be fitted directly to the data including a lag-effect.


\bibliographystyle{plain}
\bibliography{eventTools}

\end{document}

